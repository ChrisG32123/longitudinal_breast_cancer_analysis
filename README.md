# ISPY2 Multi-Sequential Modeling

*From my last project--see the `glioma_analysis` project--I haven't been able to make much progress due to the small data nature of the dataset. This dataset expands on the glioma project by studying the ISPY2 dataset.*

### 0. Setting Up The Project

0a. *Directory Strucutre:* 

I have a few lessons up my sleeve from the glioma project. In order not to overcomplicate the project, I have opted to create a few basic directories to start in my home directory `/mnt/home/gerlac37/ISPY2/`:
    - `logs/` which will hold all .err and .log files generated by the SLURM scripts when running jobs
    - `src/` which will hold all scripts

Second, I am now in big data territory, so being mindful of HPCC space and file limits is paramount. In fact, the ISPY2 dataset holds >7 million DICOM images and takes up 4Tb which already conflicts with the 1 million file limit on the HPCC from the scratch directory and 100Gb space limit of the home directory. I will have to be careful with how I store everything, but for now all the data will be stored in asubfolder in the scratch directory: `/mnt/scratch/gerlac37/ISPY2/`

0b. *Setting Environment:* Welcome to conda...

A basic working environment has been stored in the `environment.yml` file in the home directory. I plan to overwrite this basic version once I have more of my project figured out

0c. *Configuration & Logging:* To config or not to config, that is the question

I'm not doing it yet, so who knows


### 1. Downloading the data
- TL;DR: Image pressing a download button, but actually you're banging your head against the wall for days on end. Big data is fun.
- Files: 
    - SLURM Initial Download Script: `/mnt/home/gerlac37/ISPY2/download_ispy2.sh`

1a. *Initial Download:* Downloading the ISPY2 dataset requires using the NBIA Data Retreiver (NBIA) from The Cancer Imaging Archive (TCIA). Unfortunately, all hell breaks lose when we try to download it onto the HPCC. I used a git package to get around the java GUI from the NBIA and automated the download using the SLURM script `download_ispy2.sh` and the `/mnt/home/gerlac37/ISPY2/ISPY2-Cohort1-inclu-ACRIN6698-full-manifest.tcia` manifest which is downloaded from the TCIA website (as per instructions from the NBIA instructions). 

The download stores the data in the scratch directory `/mnt/scratch/gerlac37/ISPY2/data` because no other directory has sufficient space. This means the data will be deleted every 45 days, but that's why we've got the automatic download and clean up in the bash script. The data is stored in the following format:

~~~
/mnt/scratch/gerlac37/ISPY2/data/
└── <PatientID>/ (In the form: `ACRIN-6698-<unique patient number>` or `ISPY2-<unique patient number>`)
    └── <mm-dd-yyy>/ (representing the timepoints T0/T1/T2/T3)
        ├── <SeriesUID>.zip (Compressed series data)
        └── <SeriesUID>.json (Metadata)
~~~

1b. *Organize Data Structure:* I want to organize the data so that it is stored as the following:
- Files: 
    - Initial Testing: `/mnt/home/gerlac37/ISPY2/src/organize_data.ipynb`
    - Parallelized Production Script: `organize_data.py`
    - SLURM Script: `organize_data.sh`

~~~
data_organized/
└── <PatientID>/
    └── <mm-dd-yyy>/ (representing the timepoints T0/T1/T2/T3)
        ├── <SeriesUID>/
            ├── <number>.dcm (DICOM files with series data)
            ...
            ├── <number>.dcm
            └── <SeriesUID>.json (Corresponding metadata)
        ├── (Optional from script) <SeriesUID>.zip (Invalid, corrupt zip data or missing corresponding json metadata)
        ...
        └── (Optional from script) <SeriesUID>.zip
~~~

At each time point for each patient, one of the series should have a segmentation DICOM file, as indicated by its metadata json file. 

Considerations after some experimentation: 
- I am finding that some of the zip data are not valid zip files. I will assume these files are corrupted and will be disregarded. Since these zip files correspond to entire series, these series will not be included for patients whose data has been affected.
- Some zip files don't have corresponding json files, and I'm assuming vice versa. These series will also be disregarded accordingly too as either I would be missing the series designation of the data or the series itself, respectively.
- #### QUESTION: How do we make a 'multi-sequential model' that takes in varying sequences for some patients, but not every patient has all available sequences?

To do all of this, I am currently experimenting on a test subset of the data which I have copied into `/mnt/scratch/gerlac37/ISPY2/test` which includes the first two entries of the `ACRIN-6698` and `ISPY2` datasets. The `organize_data.ipynb` script accomplishes this. 

#### A production version of was created to parallelize the organization of the data called `organize_data.py` which can be ran from the SLURM script `organize_data.sh`. 

1c. *Clean Data:* Now I want to clean the remaining data. This means removing:
    (1) all invalid zip files, all unmatched zip and json files, or any other files in the <mm-dd-yy> folders that aren't already in the <SeriesUID> subfolders
    (2) any empty <mm-dd-yy> folders, ie, those that do not have any valid <SeriesUID> folders after (1)
    (4) any empty <PatientID> folders, ie, those that do not have any valid <mm-dd-yy> folders after (2)

- Files: 
    - Test Cleaning: `clean_data.ipynb`
    - Parallelized Organization & Cleaning: `clean_data.py`

#### As described above, this section was meant to clean the data that was organized. However, since I very, very quickly hit the file limit on the HPCC, I instead chose to combine the organization and cleaning sections of the code. Now, `clean_data.ipynb` both organizes and cleans the data.

Now, all data has been organized into the following structure:

~~~
data_organized/
└── <PatientID>/
    └── <mm-dd-yyy>/ (representing the timepoints T0/T1/T2/T3)
        └── <SeriesUID>/
            ├── <number>.dcm (DICOM files with series data)
            ...
            ├── <number>.dcm
            └── <SeriesUID>.json (Corresponding metadata)
~~~

1d. *Big Data Problems:* 

### By opening up every DICOM file, we go from on the order of 43,000 files representing series to millions of files--HPCC admin must have been hating me when I was uncompressing the zip files.

So, we updated the cleaning script so that it stores DICOM series into single NIfTI files. Problem solved. Now, we only have `clean_data.py` for data preprocessing after data download which organizes files into the following sturcture:

~~~
data_cleaned/
└── <PatientID>/
    └── <mm-dd-yyy>/ (representing the timepoints T0/T1/T2/T3)
        └── <SeriesUID>/
            ├── <SeriesUID>.nii.gz
            └── <SeriesUID>.json (Corresponding metadata)
~~~

We are (cross your fingers) officially ready to create our pipelines.

## BME 891: Image Evaluation & Analysis
For Alessio's Image Evaluation & Analysis class, I want to do a preliminary 'check' on the ISPY2 data. On the TCIA website, we can download several spreadsheets (xlsx) that hold radiomics information and clinical information. After talking with Alessio, I plan to do the following:
1. Calculate my own radiomics from the the patients and compare them to the radiomics files on the website
2. Create a machine learning model to classify the breast cancer among the patients using both radiomics and clinical xlsx data

### 2. Checking Radiomics & Mask Equivalence

2a. *Downloading Radiomics Data:* Way easier than the last download

Simply download the xlsx file locally and secure copy into HPCC in `data/Multi-feature-MRI-NACT-Data.xlsx`. It's amazing how simply trying to Kb instead of Tb makes your day thousands of times easier.

2b. *Aggregate Radiomics:* From specific patient IDs as listed in the xlsx, I want to extract specific radiomics from the mask that conincide with the types of radiomics extracted in the xlsx file.

# CURRENTLY HERE #

2c. *Compare Radiomics Exams:* About as descriptive as you could expect


### 3. Preprocessing Radiomics Data

3a.


### 4. Radiomics Machine Learning

4a. 


## Research Project